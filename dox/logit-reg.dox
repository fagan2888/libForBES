/*! \page doc-logit Log-logistic Regression
 *  \brief Sparse Log-Logistic regression problems
 *
 * <div class="sec" id="l1-reg">\f$\ell_1\f$-regularized</div>
 *
 * <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification#Logistic_loss">Log-logistic loss</a>
 * is a popular loss function used widely in classification problems in machine learning 
 * and statistics. 
 *
 * The \link LogLogisticLoss log-logistic\endlink loss function is given by
 *
 * \f[
 *  L_\mu(x) = \mu \sum_{i=1}^{n} -\ln \sigma(x_i),
 * \f]
 *
 * where \f$\sigma(\tau)=e^{\tau}/(1+e^{\tau})\f$. 
 *
 * The log-logistic \f$\ell_1\f$-regularized optimization problem is formulated
 * as follows
 *
 * \f[
 * \mathrm{minimize}\ \ L_{1.0}(Ax-b) + \lambda \|x\|_1,
 * \f]
 *
 *
 * where \f$\lambda>0\f$ is a regularization parameter.
 *
 * The term \f$\lambda\|x\|_1\f$ is used to enforce sparsity in \f$x\f$.
 * 
 * Let us see how one can formulate such an optimization problem and solve it
 * using libforbes using 
 * 
 * \f[
 *  A = \begin{bmatrix}
 *   1& -2&  3& -4& 5\\
 *   2& -1&  0& -1& 3\\
 *  -1&  0&  4& -3& 2\\
 *  -1& -1& -1&  1& 3 
 * \end{bmatrix}
 * \f]
 *
 * and
 *
 * \f[
 *  b = \begin{bmatrix}
 *   1&2&3&4
 *  \end{bmatrix}^\top
 * \f]
 * 
 * \code{.cpp}
 * const size_t n = 5;
 * const size_t m = 4;
 * double data_A[] = {
 *   1,  2, -1, -1,
 *  -2, -1,  0, -1,
 *   3,  0,  4, -1,
 *  -4, -1, -3,  1,
 *   5,  3,  2,  3 };
 *   double data_minus_b[] = {-1, -2, -3, -4};
 *   Matrix A(m, n, data_A);
 *   Matrix minus_b(m, 1, data_minus_b);
 * \endcode
 *
 * We should then define the problem's functions and linear operator \f$A:\mathbb{R}^5 \to \mathbb{R}^4\f$
 *
 * \code{.cpp}
 *  LinearOperator * OpA = new MatrixOperator(A);
 *  Function * f = new LogLogisticLoss();
 *  double lambda = 3.50;
 *  Function * g = new Norm1(lambda);
 * \endcode
 * 
 * It is now straightfoward to construct our optimization problem and solve it
 * as follows
 * 
 * \code{.cpp}
 *  FBProblem prob = FBProblem(*f, *OpA, minus_b, *g);
 * 
 *  // Specify an initial guess
 *  Matrix x0(n, 1);
 *
 *  // Specify the parameter gamma
 *  double gamma = 0.1;
 *
 *  // Construct a new instance of a solver (e.g., FBSplitting or FBSplittingFast)
 *  FBSplittingFast * solver = new FBSplittingFast(prob, x0, gamma);
 *
 *  // Run the solver and get the solution
 *  solver->run();
 *  Matrix xstar = solver->getSolution();
 * \endcode
 * 
 * The minimizer of the above problem is then found to be
 *
 * \f[
 * x^\star = \begin{bmatrix} 0&       0&       0&       0&   1.228\end{bmatrix}^\top,
 * \f]
 * 
 * in \f$29\f$ iterations.
 *
 *
 * <div class="sec" id="l2-reg">\f$\ell_2\f$--regularized</div>
 *
 * Similarly, we may formulate a L2-regularized logistic regression problem.
 *
 * \f[
 * \mathrm{minimize}\ \ \sum_{i=1}^{n} -\ln \sigma(z_i) + \frac{\lambda}{2} \|x\|_2,
 * \f]
 *
 * subject to \f$z=Ax-b\f$.
 * 
 * The only necessary modification in the above code is
 *
 * \code{.cpp}
 *  Function * g = new Norm2(lambda/2.0);
 * \endcode
 *
 * for \f$\lambda=6.5\f$ and using the fast variant of %FBSplitting, 
 * FBSplittingFast, we compute the minimizer
 *
 * \f[
 * x^\star = \begin{bmatrix} -0.2037&-0.304&0.2162&-0.1751&1.135\end{bmatrix}^{\top}
 * \f]
 *
 * in \f$53\f$ iterations, whereas, FBSplitting converges to the solution in 
 * \f$41\f$ iterations.
 *
 */